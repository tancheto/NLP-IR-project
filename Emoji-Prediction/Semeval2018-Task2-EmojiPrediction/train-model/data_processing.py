'''
---- Data processing -----
Given a data file with list of pairs - tweet id and origin tweet.
'''

import re
import emojilib

origin_file_path = '../data/train/{}_train.origin'
labels_file_path = '../data/train/{}_train.lables'
texts_file_path = '../data/train/{}_train.text'


def clean_text(text):
    # remove emojis, links and user mentions
    clean = ""
    text = emojilib.replace_emoji(text, replacement=' ')
    text = re.sub('\s+', ' ', text).strip()
    for word in text.split(" "):
        if word.startswith('@') or word.startswith('http'):
            pass
        else:
            clean += word + " "

    # remove double spaces
    return re.sub(' +', ' ', clean).strip()


mapping = {'us': {'â¤': '0', 'ğŸ˜': '1', 'ğŸ˜‚': '2', 'ğŸ’•': '3', 'ğŸ”¥': '4', 'ğŸ˜Š': '5', 'ğŸ˜': '6', 'âœ¨': '7', 'ğŸ’™': '8', 'ğŸ˜˜': '9', 'ğŸ“·': '10', 'ğŸ‡ºğŸ‡¸': '11', 'â˜€': '12', 'ğŸ’œ': '13', 'ğŸ˜‰': '14', 'ğŸ’¯': '15', 'ğŸ˜': '16', 'ğŸ„': '17', 'ğŸ“¸': '18', 'ğŸ˜œ': '19'},
           'es': {'â¤': '0', 'ğŸ˜': '1', 'ğŸ˜‚': '2', 'ğŸ’•': '3', 'ğŸ˜Š': '4', 'ğŸ˜˜': '5', 'ğŸ’ª': '6', 'ğŸ˜‰': '7', 'ğŸ‘Œ': '8', 'ğŸ‡ªğŸ‡¸': '9', 'ğŸ˜': '10', 'ğŸ’™': '11', 'ğŸ’œ': '12', 'ğŸ˜œ': '13', 'ğŸ’': '14', 'âœ¨': '15', 'ğŸ¶': '16', 'ğŸ’˜': '17', 'ğŸ˜': '18', '	': '19'}}


def data_processing(lang):
    labels = open(labels_file_path.format(lang), 'w', encoding="utf8")
    texts = open(texts_file_path.format(lang), 'w', encoding="utf8")

    number = 10

    with open(origin_file_path.format(lang), encoding="utf8") as origin_file:
        for line in origin_file:
            number -= 1
            if number <= 0:
                break

            text = line.split('\t')[1]
            emo_list = emojilib.emoji_list(text)
            emo_set = set([d['code'] for d in emo_list if 'code' in d])

            if len(emo_set) > 0:
                for emoji in emo_set:
                    if emoji in mapping[lang]:
                        labels.write(emoji + ' ')

                labels.write('\n')
                texts.write(clean_text(text) + '\n')
